name: üöÄ Comprehensive CI Pipeline

on:
  push:
    branches:
      - main
      - master
      - develop
      - 'cursor/**'
      - 'feature/**'
  pull_request:
    branches:
      - main
      - master
      - develop
  workflow_dispatch:
    inputs:
      run_e2e:
        description: 'Run E2E tests'
        required: false
        default: 'true'
        type: boolean
      environment:
        description: 'Target environment'
        required: false
        default: 'ci'
        type: choice
        options:
          - ci
          - staging
          - production

env:
  NODE_VERSION: '20.x'
  REPORTS_DIR: 'ci-reports'
  ARTIFACTS_RETENTION_DAYS: 30

jobs:
  # ============================================================================
  # Job 1: Setup and Validation
  # ============================================================================
  setup-validation:
    name: üìã Setup & Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
      should-run-e2e: ${{ steps.check-changes.outputs.should-run-e2e }}
      affected-areas: ${{ steps.check-changes.outputs.affected-areas }}
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better change detection
      
      - name: üîë Generate cache key
        id: cache-key
        run: |
          HASH="${{ hashFiles('package-lock.json') }}"
          echo "key=node-modules-${{ runner.os }}-${HASH}" >> $GITHUB_OUTPUT
          echo "hash=${HASH}" >> $GITHUB_OUTPUT
      
      - name: üîç Detect changes
        id: check-changes
        run: |
          # Create JSON output for affected areas
          mkdir -p ${{ env.REPORTS_DIR }}/metadata
          
          # Check what files changed
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            CHANGED_FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }})
          else
            CHANGED_FILES=$(git diff --name-only HEAD^ HEAD)
          fi
          
          # Analyze changes
          FRONTEND_CHANGED=false
          BACKEND_CHANGED=false
          CONFIG_CHANGED=false
          TEST_CHANGED=false
          
          echo "$CHANGED_FILES" | while read file; do
            case "$file" in
              src/components/*|src/views/*|src/styles/*) FRONTEND_CHANGED=true ;;
              src/server.ts|src/routes/*|src/controllers/*|src/services/*) BACKEND_CHANGED=true ;;
              package*.json|tsconfig*.json|vite.config.ts|playwright.config.ts) CONFIG_CHANGED=true ;;
              tests/*|e2e/*|**/*.test.ts|**/*.spec.ts) TEST_CHANGED=true ;;
            esac
          done
          
          # Determine if E2E should run
          SHOULD_RUN_E2E="false"
          if [ "${{ github.event.inputs.run_e2e }}" == "true" ] || \
             [ "$FRONTEND_CHANGED" == "true" ] || \
             [ "$BACKEND_CHANGED" == "true" ] || \
             [ "${{ github.ref }}" == "refs/heads/main" ] || \
             [ "${{ github.ref }}" == "refs/heads/master" ]; then
            SHOULD_RUN_E2E="true"
          fi
          
          echo "should-run-e2e=${SHOULD_RUN_E2E}" >> $GITHUB_OUTPUT
          
          # Create JSON report
          cat > ${{ env.REPORTS_DIR }}/metadata/changes.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "event": "${{ github.event_name }}",
            "changes": {
              "frontend": ${FRONTEND_CHANGED},
              "backend": ${BACKEND_CHANGED},
              "config": ${CONFIG_CHANGED},
              "tests": ${TEST_CHANGED}
            },
            "decisions": {
              "run_e2e": ${SHOULD_RUN_E2E}
            },
            "changed_files": $(echo "$CHANGED_FILES" | jq -R -s -c 'split("\n")[:-1]')
          }
          EOF
          
          echo "affected-areas=$(cat ${{ env.REPORTS_DIR }}/metadata/changes.json | jq -c .)" >> $GITHUB_OUTPUT
      
      - name: üì§ Upload metadata
        uses: actions/upload-artifact@v4
        with:
          name: ci-metadata
          path: ${{ env.REPORTS_DIR }}/metadata/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 2: Code Quality Checks
  # ============================================================================
  code-quality:
    name: üîç Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: setup-validation
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üì¶ Install dependencies
        run: npm ci
      
      - name: üîç Run ESLint
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/quality
          npm run lint -- --format json --output-file ${{ env.REPORTS_DIR }}/quality/eslint-report.json || true
          npm run lint -- --format stylish
        continue-on-error: true
      
      - name: üîç Type checking
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/quality
          npm run typecheck > ${{ env.REPORTS_DIR }}/quality/typecheck-output.txt 2>&1 || true
          
          # Create JSON summary
          cat > ${{ env.REPORTS_DIR }}/quality/typecheck-report.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "status": "$([[ $? -eq 0 ]] && echo 'passed' || echo 'failed')",
            "command": "npm run typecheck",
            "output_file": "typecheck-output.txt"
          }
          EOF
      
      - name: üìä Code complexity analysis
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/quality
          
          # Count files and lines
          TOTAL_TS_FILES=$(find src -name "*.ts" -o -name "*.tsx" | wc -l)
          TOTAL_LINES=$(find src -name "*.ts" -o -name "*.tsx" -exec wc -l {} + | tail -1 | awk '{print $1}')
          
          # Find large files (>500 lines)
          LARGE_FILES=$(find src -name "*.ts" -o -name "*.tsx" -exec wc -l {} + | awk '$1 > 500 {print}' | head -20)
          
          cat > ${{ env.REPORTS_DIR }}/quality/complexity-report.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "metrics": {
              "total_typescript_files": ${TOTAL_TS_FILES},
              "total_lines_of_code": ${TOTAL_LINES},
              "average_lines_per_file": $((TOTAL_LINES / TOTAL_TS_FILES))
            },
            "warnings": {
              "large_files_count": $(echo "$LARGE_FILES" | wc -l),
              "large_files": $(echo "$LARGE_FILES" | jq -R -s -c 'split("\n")[:-1]')
            }
          }
          EOF
      
      - name: üì§ Upload quality reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-reports
          path: ${{ env.REPORTS_DIR }}/quality/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 3: Unit Tests
  # ============================================================================
  unit-tests:
    name: üß™ Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: setup-validation
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üì¶ Install dependencies
        run: npm ci
      
      - name: üß™ Run unit tests with coverage
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/tests
          npm run test:coverage -- --reporter=json --outputFile=${{ env.REPORTS_DIR }}/tests/vitest-results.json
        env:
          CI: true
      
      - name: üìä Generate test summary JSON
        if: always()
        run: |
          # Create comprehensive test summary
          cat > ${{ env.REPORTS_DIR }}/tests/summary.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "type": "unit",
            "framework": "vitest",
            "status": "$([[ $? -eq 0 ]] && echo 'passed' || echo 'failed')",
            "coverage_dir": "coverage/",
            "results_file": "vitest-results.json"
          }
          EOF
      
      - name: üì§ Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results
          path: |
            ${{ env.REPORTS_DIR }}/tests/
            coverage/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 4: Integration Tests
  # ============================================================================
  integration-tests:
    name: üîó Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: setup-validation
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üì¶ Install dependencies
        run: npm ci
      
      - name: üîó Run integration tests
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/integration
          
          # Run API tests
          npm run test:api -- --output ${{ env.REPORTS_DIR }}/integration --format json || true
          
          # Run health checks
          npm run health:check:json || true
          
          # Create summary
          cat > ${{ env.REPORTS_DIR }}/integration/summary.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "type": "integration",
            "tests_run": [
              "api_tests",
              "health_checks"
            ],
            "status": "completed"
          }
          EOF
      
      - name: üì§ Upload integration test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: ${{ env.REPORTS_DIR }}/integration/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 5: E2E Tests
  # ============================================================================
  e2e-tests:
    name: üé≠ E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [setup-validation, unit-tests]
    if: needs.setup-validation.outputs.should-run-e2e == 'true'
    
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2]
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üì¶ Install dependencies
        run: npm ci
      
      - name: üé≠ Install Playwright browsers
        run: npx playwright install --with-deps chromium
      
      - name: üé≠ Run E2E tests
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/e2e
          npx playwright test --shard=${{ matrix.shard }}/2 --reporter=json --output=${{ env.REPORTS_DIR }}/e2e/results-shard-${{ matrix.shard }}.json
        env:
          CI: true
          NODE_OPTIONS: '--dns-result-order=ipv4first'
      
      - name: üì§ Upload E2E results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-shard-${{ matrix.shard }}
          path: |
            ${{ env.REPORTS_DIR }}/e2e/
            playwright-report/
            test-results/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 6: Build Verification
  # ============================================================================
  build-verification:
    name: üèóÔ∏è Build Verification
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [code-quality, unit-tests]
    
    strategy:
      matrix:
        build-type: [client, server]
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üì¶ Install dependencies
        run: npm ci
      
      - name: üèóÔ∏è Build ${{ matrix.build-type }}
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/build
          
          START_TIME=$(date +%s)
          npm run build:${{ matrix.build-type }} 2>&1 | tee ${{ env.REPORTS_DIR }}/build/${{ matrix.build-type }}-build.log
          BUILD_STATUS=$?
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          # Get build size
          if [ "${{ matrix.build-type }}" == "client" ]; then
            BUILD_SIZE=$(du -sh dist/ | cut -f1)
            FILE_COUNT=$(find dist/ -type f | wc -l)
          else
            BUILD_SIZE=$(du -sh dist/ | cut -f1)
            FILE_COUNT=$(find dist/ -type f -name "*.js" | wc -l)
          fi
          
          # Create build report
          cat > ${{ env.REPORTS_DIR }}/build/${{ matrix.build-type }}-report.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "build_type": "${{ matrix.build-type }}",
            "status": "$([[ $BUILD_STATUS -eq 0 ]] && echo 'success' || echo 'failed')",
            "duration_seconds": ${DURATION},
            "build_size": "${BUILD_SIZE}",
            "file_count": ${FILE_COUNT},
            "node_version": "${{ env.NODE_VERSION }}",
            "log_file": "${{ matrix.build-type }}-build.log"
          }
          EOF
          
          exit $BUILD_STATUS
      
      - name: üì§ Upload build artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: build-${{ matrix.build-type }}
          path: dist/
          retention-days: 7
      
      - name: üì§ Upload build reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: build-reports-${{ matrix.build-type }}
          path: ${{ env.REPORTS_DIR }}/build/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 7: Docker Build
  # ============================================================================
  docker-build:
    name: üê≥ Docker Build
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: build-verification
    
    strategy:
      matrix:
        image: [backend, frontend]
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üê≥ Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: üê≥ Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile.${{ matrix.image }}
          push: false
          tags: dreammaker-${{ matrix.image }}:ci-${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/${{ matrix.image }}-image.tar
      
      - name: üîç Analyze image
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/docker
          
          # Load image
          docker load --input /tmp/${{ matrix.image }}-image.tar
          
          # Get image info
          IMAGE_SIZE=$(docker image inspect dreammaker-${{ matrix.image }}:ci-${{ github.sha }} --format='{{.Size}}')
          IMAGE_LAYERS=$(docker image inspect dreammaker-${{ matrix.image }}:ci-${{ github.sha }} --format='{{len .RootFS.Layers}}')
          CREATED=$(docker image inspect dreammaker-${{ matrix.image }}:ci-${{ github.sha }} --format='{{.Created}}')
          
          # Create report
          cat > ${{ env.REPORTS_DIR }}/docker/${{ matrix.image }}-report.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "image": "${{ matrix.image }}",
            "tag": "ci-${{ github.sha }}",
            "size_bytes": ${IMAGE_SIZE},
            "size_mb": $((IMAGE_SIZE / 1024 / 1024)),
            "layers": ${IMAGE_LAYERS},
            "created": "${CREATED}",
            "dockerfile": "Dockerfile.${{ matrix.image }}"
          }
          EOF
      
      - name: üß™ Test container
        run: |
          # Start container
          CONTAINER_ID=$(docker run -d dreammaker-${{ matrix.image }}:ci-${{ github.sha }})
          
          # Wait for health check
          sleep 10
          
          # Check if running
          CONTAINER_STATUS=$(docker inspect $CONTAINER_ID --format='{{.State.Status}}')
          
          # Get logs
          docker logs $CONTAINER_ID > ${{ env.REPORTS_DIR }}/docker/${{ matrix.image }}-startup.log 2>&1
          
          # Stop container
          docker stop $CONTAINER_ID
          docker rm $CONTAINER_ID
          
          # Update report
          jq --arg status "$CONTAINER_STATUS" '.container_test = {"status": $status, "log_file": "${{ matrix.image }}-startup.log"}' \
            ${{ env.REPORTS_DIR }}/docker/${{ matrix.image }}-report.json > tmp.json && \
            mv tmp.json ${{ env.REPORTS_DIR }}/docker/${{ matrix.image }}-report.json
      
      - name: üì§ Upload Docker reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: docker-reports-${{ matrix.image }}
          path: ${{ env.REPORTS_DIR }}/docker/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 8: Security Scanning
  # ============================================================================
  security-scan:
    name: üîí Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: setup-validation
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üîí Run npm audit
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/security
          npm audit --json > ${{ env.REPORTS_DIR }}/security/npm-audit.json || true
          npm audit || true
      
      - name: üîç Check for secrets
        run: |
          # Simple secret detection
          SUSPICIOUS_PATTERNS="(password|secret|api[_-]?key|token|auth).*=.*['\"][^'\"]{8,}"
          
          FOUND_SECRETS=$(grep -r -i -E "$SUSPICIOUS_PATTERNS" src/ --exclude-dir=node_modules || true)
          
          cat > ${{ env.REPORTS_DIR }}/security/secrets-scan.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "scan_pattern": "${SUSPICIOUS_PATTERNS}",
            "findings_count": $(echo "$FOUND_SECRETS" | grep -c . || echo 0),
            "status": "$([[ -z "$FOUND_SECRETS" ]] && echo 'clean' || echo 'potential_issues')"
          }
          EOF
      
      - name: üì§ Upload security reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: ${{ env.REPORTS_DIR }}/security/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 9: Performance Benchmarks
  # ============================================================================
  performance-benchmarks:
    name: ‚ö° Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: build-verification
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: üì¶ Install dependencies
        run: npm ci
      
      - name: ‚ö° Run benchmarks
        run: |
          mkdir -p ${{ env.REPORTS_DIR }}/performance
          
          # Build time benchmark
          START=$(date +%s)
          npm run build:client > /dev/null 2>&1
          CLIENT_BUILD_TIME=$(($(date +%s) - START))
          
          START=$(date +%s)
          npm run build:server > /dev/null 2>&1
          SERVER_BUILD_TIME=$(($(date +%s) - START))
          
          # Bundle size analysis
          CLIENT_SIZE=$(du -sb dist/ | cut -f1)
          JS_SIZE=$(find dist/ -name "*.js" -exec cat {} + | wc -c)
          CSS_SIZE=$(find dist/ -name "*.css" -exec cat {} + | wc -c)
          
          cat > ${{ env.REPORTS_DIR }}/performance/benchmarks.json <<EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "build_times": {
              "client_seconds": ${CLIENT_BUILD_TIME},
              "server_seconds": ${SERVER_BUILD_TIME},
              "total_seconds": $((CLIENT_BUILD_TIME + SERVER_BUILD_TIME))
            },
            "bundle_sizes": {
              "total_bytes": ${CLIENT_SIZE},
              "total_mb": $(echo "scale=2; ${CLIENT_SIZE} / 1024 / 1024" | bc),
              "javascript_bytes": ${JS_SIZE},
              "javascript_kb": $(echo "scale=2; ${JS_SIZE} / 1024" | bc),
              "css_bytes": ${CSS_SIZE},
              "css_kb": $(echo "scale=2; ${CSS_SIZE} / 1024" | bc)
            }
          }
          EOF
      
      - name: üì§ Upload performance reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports
          path: ${{ env.REPORTS_DIR }}/performance/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}

  # ============================================================================
  # Job 10: Final Report Generation
  # ============================================================================
  generate-final-report:
    name: üìä Generate Final Report
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: 
      - setup-validation
      - code-quality
      - unit-tests
      - integration-tests
      - build-verification
      - docker-build
      - security-scan
      - performance-benchmarks
    if: always()
    
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4
      
      - name: üì• Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-reports/
      
      - name: üìä Generate comprehensive report
        run: |
          mkdir -p final-report
          
          # Aggregate all JSON reports
          cat > final-report/ci-report.json <<EOF
          {
            "pipeline": {
              "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "workflow_run_id": "${{ github.run_id }}",
              "workflow_run_number": "${{ github.run_number }}",
              "commit": "${{ github.sha }}",
              "branch": "${{ github.ref_name }}",
              "event": "${{ github.event_name }}",
              "actor": "${{ github.actor }}"
            },
            "jobs": {
              "setup_validation": "${{ needs.setup-validation.result }}",
              "code_quality": "${{ needs.code-quality.result }}",
              "unit_tests": "${{ needs.unit-tests.result }}",
              "integration_tests": "${{ needs.integration-tests.result }}",
              "build_verification": "${{ needs.build-verification.result }}",
              "docker_build": "${{ needs.docker-build.result }}",
              "security_scan": "${{ needs.security-scan.result }}",
              "performance_benchmarks": "${{ needs.performance-benchmarks.result }}"
            },
            "summary": {
              "total_jobs": 8,
              "passed": $(echo '${{ toJSON(needs.*.result) }}' | jq -r '. | map(select(. == "success")) | length'),
              "failed": $(echo '${{ toJSON(needs.*.result) }}' | jq -r '. | map(select(. == "failure")) | length'),
              "skipped": $(echo '${{ toJSON(needs.*.result) }}' | jq -r '. | map(select(. == "skipped")) | length')
            },
            "artifacts_location": "all-reports/",
            "reports_available": [
              "quality-reports",
              "unit-test-results",
              "integration-test-results",
              "build-reports",
              "docker-reports",
              "security-reports",
              "performance-reports"
            ]
          }
          EOF
          
          # Create human-readable summary
          cat > final-report/SUMMARY.md <<EOF
          # CI Pipeline Report
          
          **Run ID:** ${{ github.run_id }}  
          **Commit:** ${{ github.sha }}  
          **Branch:** ${{ github.ref_name }}  
          **Timestamp:** $(date -u +%Y-%m-%dT%H:%M:%SZ)
          
          ## Job Results
          
          | Job | Status |
          |-----|--------|
          | Setup & Validation | ${{ needs.setup-validation.result }} |
          | Code Quality | ${{ needs.code-quality.result }} |
          | Unit Tests | ${{ needs.unit-tests.result }} |
          | Integration Tests | ${{ needs.integration-tests.result }} |
          | Build Verification | ${{ needs.build-verification.result }} |
          | Docker Build | ${{ needs.docker-build.result }} |
          | Security Scan | ${{ needs.security-scan.result }} |
          | Performance Benchmarks | ${{ needs.performance-benchmarks.result }} |
          
          ## Quick Links
          
          - [Workflow Run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - [Commit](https://github.com/${{ github.repository }}/commit/${{ github.sha }})
          
          ## Next Steps
          
          $(if [ "${{ needs.code-quality.result }}" != "success" ]; then echo "- ‚ö†Ô∏è Fix code quality issues"; fi)
          $(if [ "${{ needs.unit-tests.result }}" != "success" ]; then echo "- ‚ö†Ô∏è Fix failing unit tests"; fi)
          $(if [ "${{ needs.build-verification.result }}" != "success" ]; then echo "- ‚ö†Ô∏è Fix build errors"; fi)
          $(if [ "${{ needs.security-scan.result }}" != "success" ]; then echo "- ‚ö†Ô∏è Address security vulnerabilities"; fi)
          
          EOF
      
      - name: üì§ Upload final report
        uses: actions/upload-artifact@v4
        with:
          name: ci-final-report
          path: final-report/
          retention-days: ${{ env.ARTIFACTS_RETENTION_DAYS }}
      
      - name: üìä Add to job summary
        run: |
          cat final-report/SUMMARY.md >> $GITHUB_STEP_SUMMARY
      
      - name: üí¨ Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('final-report/SUMMARY.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
      
      - name: ‚úÖ Pipeline Success
        if: success()
        run: echo "‚úÖ All CI checks passed!"
      
      - name: ‚ùå Pipeline Failed
        if: failure()
        run: |
          echo "‚ùå CI pipeline failed. Check the reports for details."
          exit 1
